{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch.nn.init import xavier_normal_\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict as ddict\n",
    "from ordered_set import OrderedSet\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np, sys, os, json,argparse\n",
    "from pprint import pprint\n",
    "import logging\n",
    "import inspect\n",
    "from torch_scatter import scatter\n",
    "import networkx as nx \n",
    "from torch_geometric.nn import GCNConv\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param(shape):\n",
    "\tparam = Parameter(torch.Tensor(*shape)); \t\n",
    "\txavier_normal_(param.data)\n",
    "\treturn param\n",
    "\n",
    "def set_gpu(gpus):\n",
    "\n",
    "\tos.environ[\"CUDA_DEVICE_ORDER\"]    = \"PCI_BUS_ID\"\n",
    "\tos.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus\n",
    "      \n",
    "def get_logger(name, log_dir, config_dir):\n",
    "\n",
    "\tconfig_dict = json.load(open( config_dir + 'log_config.json'))\n",
    "\tconfig_dict['handlers']['file_handler']['filename'] = log_dir + name.replace('/', '-')\n",
    "\tlogging.config.dictConfig(config_dict)\n",
    "\tlogger = logging.getLogger(name)\n",
    "\n",
    "\tstd_out_format = '%(asctime)s - [%(levelname)s] - %(message)s'\n",
    "\tconsoleHandler = logging.StreamHandler(sys.stdout)\n",
    "\tconsoleHandler.setFormatter(logging.Formatter(std_out_format))\n",
    "\tlogger.addHandler(consoleHandler)\n",
    "\n",
    "\treturn logger\n",
    "\n",
    "def conj(a):\n",
    "\ta[..., 1] = -a[..., 1]\n",
    "\treturn a\n",
    "\n",
    "def com_mult(a, b):\n",
    "\tr1, i1 = a[..., 0], a[..., 1]\n",
    "\tr2, i2 = b[..., 0], b[..., 1]\n",
    "\treturn torch.stack([r1 * r2 - i1 * i2, r1 * i2 + i1 * r2], dim = -1)\n",
    "\n",
    "def ccorr(a, b):\n",
    "      A = torch.fft.rfft(a, dim=-1)\n",
    "      B = torch.fft.rfft(b, dim=-1)\n",
    "      out = torch.fft.irfft(A.conj() * B, n=a.shape[-1], dim=-1)\n",
    "      return out\n",
    "\n",
    "def get_combined_results(left_results, right_results):\n",
    "\n",
    "\n",
    "\tresults = {}\n",
    "\tcount   = float(left_results['count'])\n",
    "\n",
    "\tresults['left_mr']\t= round(left_results ['mr'] /count, 5)\n",
    "\tresults['left_mrr']\t= round(left_results ['mrr']/count, 5)\n",
    "\tresults['right_mr']\t= round(right_results['mr'] /count, 5)\n",
    "\tresults['right_mrr']\t= round(right_results['mrr']/count, 5)\n",
    "\tresults['mr']\t\t= round((left_results['mr']  + right_results['mr']) /(2*count), 5)\n",
    "\tresults['mrr']\t\t= round((left_results['mrr'] + right_results['mrr'])/(2*count), 5)\n",
    "\n",
    "\tfor k in (0,2,9):\n",
    "\t\tresults['left_hits@{}'.format(k+1)]\t= round(left_results ['hits@{}'.format(k+1)]/count, 5)\n",
    "\t\tresults['right_hits@{}'.format(k+1)]\t= round(right_results['hits@{}'.format(k+1)]/count, 5)\n",
    "\t\tresults['hits@{}'.format(k+1)]\t\t= round((left_results['hits@{}'.format(k+1)] + right_results['hits@{}'.format(k+1)])/(2*count), 5)\n",
    "\treturn results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "\n",
    "\tdef __init__(self, triples, params):\n",
    "\t\tself.triples\t= triples\n",
    "\t\tself.p \t\t    = params\n",
    "\t\tself.entities\t= np.arange(self.p.num_ent, dtype=np.int32)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.triples)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tele\t\t\t= self.triples[idx]\n",
    "\t\ttriple, label, sub_samp\t= torch.LongTensor(ele['triple']), np.int32(ele['label']), np.float32(ele['sub_samp'])\n",
    "\t\ttrp_label\t\t= self.get_label(label)\n",
    "\n",
    "\t\tif self.p.lbl_smooth != 0.0:\n",
    "\t\t\ttrp_label = (1.0 - self.p.lbl_smooth)*trp_label + (1.0/self.p.num_ent)\n",
    "\n",
    "\t\treturn triple, trp_label, None, None\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef collate_fn(data):\n",
    "\t\ttriple\t\t= torch.stack([_[0] \tfor _ in data], dim=0)\n",
    "\t\ttrp_label\t= torch.stack([_[1] \tfor _ in data], dim=0)\n",
    "\t\treturn triple, trp_label\n",
    "\t\n",
    "\tdef get_neg_ent(self, triple, label):\n",
    "\t\tdef get(triple, label):\n",
    "\t\t\tpos_obj\t\t= label\n",
    "\t\t\tmask\t\t= np.ones([self.p.num_ent], dtype=np.bool)\n",
    "\t\t\tmask[label]\t= 0\n",
    "\t\t\tneg_ent\t\t= np.int32(np.random.choice(self.entities[mask], self.p.neg_num - len(label), replace=False)).reshape([-1])\n",
    "\t\t\tneg_ent\t\t= np.concatenate((pos_obj.reshape([-1]), neg_ent))\n",
    "\n",
    "\t\t\treturn neg_ent\n",
    "\n",
    "\t\tneg_ent = get(triple, label)\n",
    "\t\treturn neg_ent\n",
    "\n",
    "\tdef get_label(self, label):\n",
    "\t\ty = np.zeros([self.p.num_ent], dtype=np.float32)\n",
    "\t\tfor e2 in label: y[e2] = 1.0\n",
    "\t\treturn torch.FloatTensor(y)\n",
    "\t\n",
    "class TestDataset(Dataset):\n",
    "\n",
    "\tdef __init__(self, triples, params):\n",
    "\t\tself.triples\t= triples\n",
    "\t\tself.p \t\t= params\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.triples)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tele\t\t= self.triples[idx]\n",
    "\t\ttriple, label\t= torch.LongTensor(ele['triple']), np.int32(ele['label'])\n",
    "\t\tlabel\t\t= self.get_label(label)\n",
    "\n",
    "\t\treturn triple, label\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef collate_fn(data):\n",
    "\t\ttriple\t\t= torch.stack([_[0] \tfor _ in data], dim=0)\n",
    "\t\tlabel\t\t= torch.stack([_[1] \tfor _ in data], dim=0)\n",
    "\t\treturn triple, label\n",
    "\t\n",
    "\tdef get_label(self, label):\n",
    "\t\ty = np.zeros([self.p.num_ent], dtype=np.float32)\n",
    "\t\tfor e2 in label: y[e2] = 1.0\n",
    "\t\treturn torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_(name, src, index, dim_size=None):\n",
    "\n",
    "\tif name == 'add': name = 'sum'\n",
    "\tassert name in ['sum', 'mean', 'max']\n",
    "\tout = scatter(src, index, dim=0, out=None, dim_size=dim_size, reduce=name)\n",
    "\treturn out[0] if isinstance(out, tuple) else out\n",
    "\n",
    "class MessagePassing(torch.nn.Module):\n",
    "\n",
    "\n",
    "\tdef __init__(self, aggr='add'):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.message_args = inspect.getfullargspec(self.message)[0][1:]\t# In the defined message function: get the list of arguments as list of string| For eg. in rgcn this will be ['x_j', 'edge_type', 'edge_norm'] (arguments of message function)\n",
    "\t\tself.update_args  = inspect.getfullargspec(self.update)[0][2:]\t# Same for update function starting from 3rd argument | first=self, second=out\n",
    "\n",
    "\tdef propagate(self, aggr, edge_index, **kwargs):\n",
    "\n",
    "\t\tassert aggr in ['add', 'mean', 'max']\n",
    "\t\tkwargs['edge_index'] = edge_index\n",
    "\n",
    "\n",
    "\t\tsize = None\n",
    "\t\tmessage_args = []\n",
    "\t\tfor arg in self.message_args:\n",
    "\t\t\tif arg[-2:] == '_i':\t\t\t\t\t# If arguments ends with _i then include indic\n",
    "\t\t\t\ttmp  = kwargs[arg[:-2]]\t\t\t\t# Take the front part of the variable | Mostly it will be 'x', \n",
    "\t\t\t\tsize = tmp.size(0)\n",
    "\t\t\t\tmessage_args.append(tmp[edge_index[0]])\t\t# Lookup for head entities in edges\n",
    "\t\t\telif arg[-2:] == '_j':\n",
    "\t\t\t\ttmp  = kwargs[arg[:-2]]\t\t\t\t# tmp = kwargs['x']\n",
    "\t\t\t\tsize = tmp.size(0)\n",
    "\t\t\t\tmessage_args.append(tmp[edge_index[1]])\t\t# Lookup for tail entities in edges\n",
    "\t\t\telse:\n",
    "\t\t\t\tmessage_args.append(kwargs[arg])\t\t# Take things from kwargs\n",
    "\n",
    "\t\tupdate_args = [kwargs[arg] for arg in self.update_args]\t\t# Take update args from kwargs\n",
    "\n",
    "\t\tout = self.message(*message_args)\n",
    "\t\tout = scatter_(aggr, out, edge_index[0], dim_size=size)\t\t# Aggregated neighbors for each vertex\n",
    "\t\tout = self.update(out, *update_args)\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\tdef message(self, x_j):  # pragma: no cover\n",
    "\n",
    "\t\treturn x_j\n",
    "\n",
    "\tdef update(self, aggr_out):  # pragma: no cover\n",
    "\n",
    "\t\treturn aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompGCNConv(MessagePassing):\n",
    "\tdef __init__(self, in_channels, out_channels, act=lambda x:x, cache=True, params=None):\n",
    "\t\tsuper(self.__class__, self).__init__()\n",
    "\n",
    "\t\tself.p \t\t\t\t= params\n",
    "\t\tself.in_channels\t= in_channels\n",
    "\t\tself.out_channels\t= out_channels\n",
    "\t\tself.act \t\t\t= act\n",
    "\t\tself.device\t\t\t= None\n",
    "\t\tself.cache \t\t\t= cache\t\t\t# Should be False for graph classification tasks\n",
    "\t\tself.b_norm \t\t= False\n",
    "\n",
    "\t\tself.w_loop\t\t\t= get_param((in_channels, out_channels))\n",
    "\t\tself.w_in\t\t\t= get_param((in_channels, out_channels))\n",
    "\t\tself.w_out\t\t\t= get_param((in_channels, out_channels))\n",
    "\n",
    "\t\tself.w_rel \t\t\t= get_param((in_channels, out_channels))\n",
    "\t\tself.loop_rel \t\t= get_param((1, in_channels))\n",
    "\n",
    "\t\tself.drop\t\t\t= torch.nn.Dropout(self.p.dropout)\n",
    "\t\tself.bn\t\t\t\t= torch.nn.BatchNorm1d(out_channels)\n",
    "\t\t# Check if b_norm is present in params and set it\n",
    "\t\tif hasattr(self.p, 'b_norm'):\n",
    "\t\t\tself.b_norm = self.p.b_norm\n",
    "\t\telse:\n",
    "\t\t\tself.b_norm = False  # Set a default value\n",
    "\n",
    "\t\tif self.p.bias: self.register_parameter('bias', Parameter(torch.zeros(out_channels)))\n",
    "\n",
    "\n",
    "\tdef forward(self, x, edge_index, edge_type, edge_norm=None, rel_embed=None):\n",
    "\t\n",
    "\t\tif self.device is None:\n",
    "\t\t\tself.device = edge_index.device\n",
    "\n",
    "\t\trel_embed = torch.cat([rel_embed, self.loop_rel], dim=0)\n",
    "\n",
    "\t\tnum_edges = edge_index.size(1) // 2\n",
    "\t\tnum_ent   = x.size(0)\n",
    "\n",
    "\t\tif not self.cache == None:\n",
    "\t\t\tself.in_index, self.out_index = edge_index[:, :num_edges], edge_index[:, num_edges:]\n",
    "\t\t\tself.in_type,  self.out_type  = edge_type[:num_edges], \t edge_type [num_edges:]\n",
    "\n",
    "\t\t\tself.loop_index  = torch.stack([torch.arange(num_ent), torch.arange(num_ent)]).to(self.device)\n",
    "\t\t\tself.loop_type   = torch.full((num_ent,), rel_embed.size(0)-1, dtype=torch.long).to(self.device)\n",
    "\n",
    "\t\t\tself.in_norm     = self.compute_norm(self.in_index,  num_ent)\n",
    "\t\t\tself.out_norm    = self.compute_norm(self.out_index, num_ent)\n",
    "\t\t\n",
    "\t\tin_res\t\t= self.propagate('add', self.in_index,   x=x, edge_type=self.in_type,   rel_embed=rel_embed, edge_norm=self.in_norm, \tmode='in')\n",
    "\t\tloop_res\t= self.propagate('add', self.loop_index, x=x, edge_type=self.loop_type, rel_embed=rel_embed, edge_norm=None, \t\t\tmode='loop')\n",
    "\t\tout_res\t\t= self.propagate('add', self.out_index,  x=x, edge_type=self.out_type,  rel_embed=rel_embed, edge_norm=self.out_norm,\tmode='out')\n",
    "\t\tout\t\t\t= self.drop(in_res)*(1/3) + self.drop(out_res)*(1/3) + loop_res*(1/3)\n",
    "\n",
    "\t\tif self.p.bias: out = out + self.bias\n",
    "\t\tif self.b_norm: out = self.bn(out)\n",
    "\n",
    "\t\treturn self.act(out), torch.matmul(rel_embed, self.w_rel)[:-1]\n",
    "\n",
    "\tdef rel_transform(self, ent_embed, rel_embed):\n",
    "\t\ttrans_embed  = ccorr(ent_embed, rel_embed)\n",
    "\t\treturn trans_embed\n",
    "\n",
    "\tdef message(self, x_j, edge_type, rel_embed, edge_norm, mode):\n",
    "\t\tweight \t= getattr(self, 'w_{}'.format(mode))\n",
    "\t\trel_emb = torch.index_select(rel_embed, 0, edge_type)\n",
    "\t\txj_rel  = self.rel_transform(x_j, rel_emb)\n",
    "\t\tout\t= torch.mm(xj_rel, weight)\n",
    "\n",
    "\t\treturn out if edge_norm is None else out * edge_norm.view(-1, 1)\n",
    "\n",
    "\tdef update(self, aggr_out):\n",
    "\t\treturn aggr_out\n",
    "\n",
    "\tdef compute_norm(self, edge_index, num_ent):\n",
    "\t\trow, col\t= edge_index\n",
    "\t\tedge_weight \t= torch.ones_like(row).float()\n",
    "\t\tdeg\t\t= scatter_add( edge_weight, row, dim=0, dim_size=num_ent)\t# Summing number of weights of the edges [Computing out-degree] [Should be equal to in-degree (undireted graph)]\n",
    "\t\tdeg_inv\t\t= deg.pow(-0.5)\t\t\t\t\t\t\t# D^{-0.5}\n",
    "\t\tdeg_inv[deg_inv\t== float('inf')] = 0\n",
    "\t\tnorm\t\t= deg_inv[row] * edge_weight * deg_inv[col]\t\t\t# D^{-0.5}\n",
    "\n",
    "\t\treturn norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(torch.nn.Module):\n",
    "\tdef __init__(self, params):\n",
    "\t\tsuper(BaseModel, self).__init__()\n",
    "\n",
    "\t\tself.p\t\t= params\n",
    "\t\tself.act\t= torch.tanh\n",
    "\t\tself.bceloss\t= torch.nn.BCELoss()\n",
    "\n",
    "\tdef loss(self, pred, true_label):\n",
    "\t\tloss_bce = self.bceloss(pred, true_label)\n",
    "\t\treturn loss_bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompGCNBase(BaseModel):\n",
    "\tdef __init__(self, edge_index, edge_type, rrg_edge_index, rrg_edge_type, features,chequer_perm, params=None):\n",
    "\t\tsuper(CompGCNBase, self).__init__(params)\n",
    "\n",
    "\t\tself.edge_index\t\t = edge_index\n",
    "\t\tself.edge_type\t\t = edge_type\n",
    "\t\tself.rrg_edge_index  = rrg_edge_index\n",
    "\t\tself.rrg_edge_type   = rrg_edge_type\n",
    "\n",
    "\t\tself.features        = features\n",
    "\t\tself.device\t\t\t= self.edge_index.device\n",
    "\t\t\n",
    "\t\tself.init_embed\t\t= get_param((self.p.num_ent,   self.p.init_dim-20))\n",
    "\t\tself.init_rel  \t\t= get_param((self.p.num_rel*2,   self.p.init_dim))\n",
    "\t\tself.init_rel_1\t\t= Parameter(self.init_rel)\n",
    "\n",
    "\t\tself.conv1 = CompGCNConv(self.p.init_dim, self.p.embed_dim, act=self.act, params=self.p)\n",
    "\t\tself.conv2 = GCNConv(self.p.init_dim, self.p.embed_dim, add_self_loops=True)\n",
    "\n",
    "\t\tself.register_parameter('bias', Parameter(torch.zeros(self.p.num_ent)))\n",
    "\n",
    "\n",
    "\t\tself.bn\t\t= torch.nn.BatchNorm1d(self.p.embed_dim)\n",
    "\n",
    "\n",
    "\n",
    "\tdef forward_base(self, sub, rel, drop1, drop2):\n",
    "\n",
    "\t\tr  = self.init_rel\n",
    "\t\tr1 = self.init_rel_1\n",
    "\t\tx    \t= torch.cat([self.init_embed, self.features], dim=1)\n",
    "\t\tx       = self.bn(x)\n",
    "\t\tr_rrg \t\t= self.conv2(r1, self.rrg_edge_index, self.rrg_edge_type)\n",
    "\t\tx_eeg, r_eeg \t= self.conv1(x, self.edge_index, self.edge_type, rel_embed=r)\n",
    "\t\tx_eeg\t \t= drop1(x_eeg)\n",
    "\n",
    "\t\tx = x_eeg\n",
    "\n",
    "\t\tr = 0.1*r_rrg + 0.9*r_eeg\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tsub_emb\t= torch.index_select(x, 0, sub)\n",
    "\t\trel_emb\t= torch.index_select(r, 0, rel)\n",
    "\n",
    "\t\treturn sub_emb, rel_emb, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompGCN_ConvE(CompGCNBase):\n",
    "\tdef __init__(self, edge_index, edge_type, rrg_edge_index, rrg_edge_type, features, chequer_perm, params=None):\n",
    "\t\tsuper(self.__class__, self).__init__(edge_index, edge_type, rrg_edge_index, rrg_edge_type, features, chequer_perm,params)\n",
    "\t\t\n",
    "\t\tself.bn0\t\t= torch.nn.BatchNorm2d(1)\n",
    "\t\tself.bn1\t\t= torch.nn.BatchNorm2d(self.p.num_filt)\n",
    "\t\tself.bn2\t\t= torch.nn.BatchNorm1d(self.p.embed_dim)\n",
    "\t\t\n",
    "\t\tself.hidden_drop\t= torch.nn.Dropout(self.p.hid_drop)\n",
    "\t\tself.hidden_drop2\t= torch.nn.Dropout(self.p.hid_drop2)\n",
    "\t\tself.feature_drop\t= torch.nn.Dropout(self.p.feat_drop)\n",
    "\t\tself.m_conv1\t\t= torch.nn.Conv2d(1, out_channels=self.p.num_filt, kernel_size=(self.p.ker_sz, self.p.ker_sz), stride=1, padding=0, bias=self.p.bias)\n",
    "\n",
    "\t\tflat_sz_h\t\t= int(2*self.p.k_w) - self.p.ker_sz + 1\n",
    "\t\tflat_sz_w\t\t= self.p.k_h \t    - self.p.ker_sz + 1\n",
    "\t\tself.flat_sz\t= flat_sz_h*flat_sz_w*self.p.num_filt\n",
    "\t\tself.fc\t\t\t= torch.nn.Linear(self.flat_sz, self.p.embed_dim)\n",
    "\n",
    "\tdef concat(self, e1_embed, rel_embed):\n",
    "\t\te1_embed\t= e1_embed. view(-1, 1, self.p.embed_dim)\n",
    "\t\trel_embed\t= rel_embed.view(-1, 1, self.p.embed_dim)\n",
    "\t\tstack_inp\t= torch.cat([e1_embed, rel_embed], 1)\n",
    "\t\tstack_inp\t= torch.transpose(stack_inp, 2, 1).reshape((-1, 1, 2*self.p.k_w, self.p.k_h))\n",
    "\t\treturn stack_inp\n",
    "\n",
    "\tdef forward(self, sub, rel):\n",
    "\n",
    "\t\tsub_emb, rel_emb, all_ent\t= self.forward_base(sub, rel, self.hidden_drop, self.feature_drop)\n",
    "\t\tstk_inp\t\t\t\t= self.concat(sub_emb, rel_emb)\n",
    "\t\tx\t\t\t\t= self.bn0(stk_inp)\n",
    "\t\tx\t\t\t\t= self.m_conv1(x)\n",
    "\t\tx\t\t\t\t= self.bn1(x)\n",
    "\t\tx\t\t\t\t= F.relu(x)\n",
    "\t\tx\t\t\t\t= self.feature_drop(x)\n",
    "\t\tx\t\t\t\t= x.view(-1, self.flat_sz)\n",
    "\t\tx\t\t\t\t= self.fc(x)\n",
    "\t\tx\t\t\t\t= self.hidden_drop2(x)\n",
    "\t\tx\t\t\t\t= self.bn2(x)\n",
    "\t\tx\t\t\t\t= F.relu(x)\n",
    "\n",
    "\t\tx = torch.mm(x, all_ent.transpose(1,0))\n",
    "\t\tx += self.bias.expand_as(x)\n",
    "\n",
    "\t\tscore = torch.sigmoid(x)\n",
    "\t\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompGCN_DistMult(CompGCNBase):\n",
    "\tdef __init__(self, edge_index, edge_type, rrg_edge_index, rrg_edge_type, features,  chequer_perm, params=None):\n",
    "\t\tsuper(self.__class__, self).__init__(edge_index, edge_type, rrg_edge_index, rrg_edge_type, features,  chequer_perm,params)\n",
    "\t\tself.drop = torch.nn.Dropout(self.p.hid_drop)\n",
    "\n",
    "\tdef forward(self, sub, rel):\n",
    "\n",
    "\t\tsub_emb, rel_emb, all_ent\t= self.forward_base(sub, rel, self.drop, self.drop)\n",
    "\t\tobj_emb\t\t\t\t= sub_emb * rel_emb\n",
    "\n",
    "\t\tx = torch.mm(obj_emb, all_ent.transpose(1, 0))\n",
    "\t\tx += self.bias.expand_as(x)\n",
    "\n",
    "\t\tscore = torch.sigmoid(x)\n",
    "\t\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompGCN_InteractE(CompGCNBase):\n",
    "    def __init__(self, edge_index, edge_type, rrg_edge_index, rrg_edge_type, features,chequer_perm, params=None):\n",
    "        super(self.__class__, self).__init__(edge_index, edge_type, rrg_edge_index, rrg_edge_type, features, chequer_perm, params)\n",
    "\n",
    "        self.dilation_rate2 = 2  \n",
    "        self.dilation_rate3 = 3 \n",
    "        \n",
    "        self.register_parameter('conv_filt_orig', Parameter(\n",
    "            torch.zeros(self.p.num_filt, 1, self.p.ker_sz, self.p.ker_sz)))\n",
    "        self.register_parameter('conv_filt_dilated2', Parameter(\n",
    "            torch.zeros(self.p.num_filt, 1, self.p.ker_sz, self.p.ker_sz)))\n",
    "        self.register_parameter('conv_filt_dilated3', Parameter(\n",
    "            torch.zeros(self.p.num_filt, 1, self.p.ker_sz, self.p.ker_sz)))\n",
    "        \n",
    "        self.alpha = 1/3  \n",
    "        self.beta = 1/3   \n",
    "        xavier_normal_(self.conv_filt_orig)\n",
    "        xavier_normal_(self.conv_filt_dilated2)\n",
    "        xavier_normal_(self.conv_filt_dilated3)\n",
    "        \n",
    "        self.inp_drop = torch.nn.Dropout(self.p.hid_drop)\n",
    "        self.hidden_drop = torch.nn.Dropout(self.p.hid_drop2)\n",
    "        self.feature_map_drop = torch.nn.Dropout2d(self.p.feat_drop)\n",
    "        self.bn0 = torch.nn.BatchNorm2d(self.p.perm)\n",
    "        \n",
    "        flat_sz_h = self.p.k_h\n",
    "        flat_sz_w = 2*self.p.k_w\n",
    "        self.padding = 0\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm2d(self.p.num_filt*self.p.perm)\n",
    "        self.flat_sz = flat_sz_h * flat_sz_w * self.p.num_filt*self.p.perm\n",
    "\n",
    "        self.bn2 = torch.nn.BatchNorm1d(self.p.embed_dim)\n",
    "        self.fc = torch.nn.Linear(self.flat_sz, self.p.embed_dim)\n",
    "        self.chequer_perm = chequer_perm\n",
    "\n",
    "        self.register_parameter('bias', Parameter(torch.zeros(self.p.num_ent)))\n",
    "        self.register_parameter('conv_filt', Parameter(torch.zeros(self.p.num_filt, 1, self.p.ker_sz,  self.p.ker_sz)))\n",
    "        xavier_normal_(self.conv_filt)\n",
    "\n",
    "    def circular_padding_chw(self, batch, padding):\n",
    "        upper_pad = batch[..., -padding:, :]\n",
    "        lower_pad = batch[..., :padding, :]\n",
    "        temp = torch.cat([upper_pad, batch, lower_pad], dim=2)\n",
    "\n",
    "        left_pad = temp[..., -padding:]\n",
    "        right_pad = temp[..., :padding]\n",
    "        padded = torch.cat([left_pad, temp, right_pad], dim=3)\n",
    "        return padded\n",
    "\n",
    "    def forward(self, sub, rel):\n",
    "        sub_emb, rel_emb, all_ent = self.forward_base(sub, rel, self.hidden_drop, self.feature_map_drop)\n",
    "        comb_emb = torch.cat([sub_emb, rel_emb], dim=1)\n",
    "        chequer_perm = comb_emb[:, self.chequer_perm]\n",
    "        stack_inp = chequer_perm.reshape((-1, self.p.perm, 2*self.p.k_w, self.p.k_h))\n",
    "        stack_inp = self.bn0(stack_inp)\n",
    "        x = self.inp_drop(stack_inp)\n",
    "\n",
    "        pad_orig = self.p.ker_sz // 2\n",
    "        x_orig = self.circular_padding_chw(x, pad_orig)\n",
    "        x_orig = F.conv2d(x_orig, \n",
    "                         self.conv_filt_orig.repeat(self.p.perm, 1, 1, 1),\n",
    "                         padding=0,\n",
    "                         groups=self.p.perm,\n",
    "                         dilation=1)\n",
    "        \n",
    "        pad_dilated2 = self.dilation_rate2 * (self.p.ker_sz // 2)\n",
    "        x_dilated2 = self.circular_padding_chw(x, pad_dilated2)\n",
    "        x_dilated2 = F.conv2d(x_dilated2, \n",
    "                            self.conv_filt_dilated2.repeat(self.p.perm, 1, 1, 1),\n",
    "                            padding=0,\n",
    "                            groups=self.p.perm,\n",
    "                            dilation=self.dilation_rate2)\n",
    "        \n",
    "        pad_dilated3 = self.dilation_rate3 * (self.p.ker_sz // 2)\n",
    "        x_dilated3 = self.circular_padding_chw(x, pad_dilated3)\n",
    "        x_dilated3 = F.conv2d(x_dilated3, \n",
    "                            self.conv_filt_dilated3.repeat(self.p.perm, 1, 1, 1),\n",
    "                            padding=0,\n",
    "                            groups=self.p.perm,\n",
    "                            dilation=self.dilation_rate3)\n",
    "        \n",
    "        gamma = 1.0 - self.alpha - self.beta\n",
    "        x = (self.alpha * x_orig + self.beta * x_dilated2 + gamma * x_dilated3)\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.feature_map_drop(x)\n",
    "        x = x.view(-1, self.flat_sz)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = self.hidden_drop(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = torch.mm(x, all_ent.transpose(1,0))\n",
    "        x += self.bias.expand_as(x)\n",
    "        score = torch.sigmoid(x)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner(object):\n",
    "\t\n",
    "\tdef __init__(self, params):\n",
    "\n",
    "\t\tself.p\t\t\t= params\n",
    "\t\tself.logger\t\t= get_logger(self.p.name, self.p.log_dir, self.p.config_dir)\n",
    "\n",
    "\t\tself.logger.info(vars(self.p))\n",
    "\t\tpprint(vars(self.p))\n",
    "\n",
    "\t\tif self.p.gpu != '-1' and torch.cuda.is_available():\n",
    "\t\t\tself.device = torch.device('cuda')\n",
    "\t\t\ttorch.cuda.set_rng_state(torch.cuda.get_rng_state())\n",
    "\t\t\ttorch.backends.cudnn.deterministic = True\n",
    "\t\telse:\n",
    "\t\t\tself.device = torch.device('cpu')\n",
    "\n",
    "\t\tself.load_data()\n",
    "\t\tself.model        = self.add_model(self.p.model, self.p.score_func)\n",
    "\t\tself.optimizer    = self.add_optimizer(self.model.parameters())\n",
    "\n",
    "\n",
    "\tdef load_data(self):\n",
    "\t\t\"\"\"\n",
    "\t\tReading in raw triples and converts it into a standard format. \n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tself.p.dataset:         Takes in the name of the dataset (FB15k-237)\n",
    "\t\t\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tself.ent2id:            Entity to unique identifier mapping\n",
    "\t\tself.id2rel:            Inverse mapping of self.ent2id\n",
    "\t\tself.rel2id:            Relation to unique identifier mapping\n",
    "\t\tself.num_ent:           Number of entities in the Knowledge graph\n",
    "\t\tself.num_rel:           Number of relations in the Knowledge graph\n",
    "\t\tself.embed_dim:         Embedding dimension used\n",
    "\t\tself.data['train']:     Stores the triples corresponding to training dataset\n",
    "\t\tself.data['valid']:     Stores the triples corresponding to validation dataset\n",
    "\t\tself.data['test']:      Stores the triples corresponding to test dataset\n",
    "\t\tself.data_iter:\t\t    The dataloader for different data splits\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tent_set, rel_set = OrderedSet(), OrderedSet()\n",
    "\t\tfor split in ['train', 'test', 'valid']:\n",
    "\t\t\tfor line in open('./data/{}/{}.txt'.format(self.p.dataset, split)):\n",
    "\t\t\t\tsub, rel, obj = map(str.lower, line.strip().split('\\t'))\n",
    "\t\t\t\tent_set.add(sub)\n",
    "\t\t\t\trel_set.add(rel)\n",
    "\t\t\t\tent_set.add(obj)\n",
    "\n",
    "\t\tself.ent2id = {ent: idx for idx, ent in enumerate(ent_set)}\n",
    "\t\tself.rel2id = {rel: idx for idx, rel in enumerate(rel_set)}\n",
    "\t\tself.rel2id.update({rel+'_reverse': idx+len(self.rel2id) for idx, rel in enumerate(rel_set)})\n",
    "\n",
    "\t\tself.id2ent = {idx: ent for ent, idx in self.ent2id.items()}\n",
    "\t\tself.id2rel = {idx: rel for rel, idx in self.rel2id.items()}\n",
    "\n",
    "\t\tself.p.num_ent\t\t= len(self.ent2id)\n",
    "\t\tself.p.num_rel\t\t= len(self.rel2id) // 2\n",
    "\t\tself.p.embed_dim\t= self.p.k_w * self.p.k_h if self.p.embed_dim is None else self.p.embed_dim\n",
    "\t\tprint('num entities: ', self.p.num_ent)\n",
    "\t\tprint('num relations: ', self.p.num_rel)\n",
    "\t\tself.data = ddict(list)\n",
    "\t\tsr2o = ddict(set)\n",
    "\n",
    "\t\tfor split in ['train', 'test', 'valid']:\n",
    "\t\t\tfor line in open('./data/{}/{}.txt'.format(self.p.dataset, split)):\n",
    "\t\t\t\tsub, rel, obj = map(str.lower, line.strip().split('\\t'))\n",
    "\t\t\t\tsub, rel, obj = self.ent2id[sub], self.rel2id[rel], self.ent2id[obj]\n",
    "\t\t\t\tself.data[split].append((sub, rel, obj))\n",
    "\n",
    "\t\t\t\tif split == 'train': \n",
    "\t\t\t\t\tsr2o[(sub, rel)].add(obj)\n",
    "\t\t\t\t\tsr2o[(obj, rel+self.p.num_rel)].add(sub)\n",
    "\n",
    "\t\tself.data = dict(self.data)\n",
    "\n",
    "\t\tself.sr2o = {k: list(v) for k, v in sr2o.items()}\n",
    "\t\tfor split in ['test', 'valid']:\n",
    "\t\t\tfor sub, rel, obj in self.data[split]:\n",
    "\t\t\t\tsr2o[(sub, rel)].add(obj)\n",
    "\t\t\t\tsr2o[(obj, rel+self.p.num_rel)].add(sub)\n",
    "\n",
    "\t\tself.sr2o_all = {k: list(v) for k, v in sr2o.items()}\n",
    "\t\tself.triples  = ddict(list)\n",
    "\n",
    "\t\tfor (sub, rel), obj in self.sr2o.items():\n",
    "\t\t\tself.triples['train'].append({'triple':(sub, rel, -1), 'label': self.sr2o[(sub, rel)], 'sub_samp': 1})\n",
    "\n",
    "\t\tfor split in ['test', 'valid']:\n",
    "\t\t\tfor sub, rel, obj in self.data[split]:\n",
    "\t\t\t\trel_inv = rel + self.p.num_rel\n",
    "\t\t\t\tself.triples['{}_{}'.format(split, 'tail')].append({'triple': (sub, rel, obj), \t   'label': self.sr2o_all[(sub, rel)]})\n",
    "\t\t\t\tself.triples['{}_{}'.format(split, 'head')].append({'triple': (obj, rel_inv, sub), 'label': self.sr2o_all[(obj, rel_inv)]})\n",
    "\n",
    "\t\tself.triples = dict(self.triples)\n",
    "\t\t\n",
    "\t\tdef get_data_loader(dataset_class, split, batch_size, shuffle=True):\n",
    "\t\t\treturn  DataLoader(\n",
    "\t\t\t\t\tdataset_class(self.triples[split], self.p),\n",
    "\t\t\t\t\tbatch_size      = batch_size,\n",
    "\t\t\t\t\tshuffle         = shuffle,\n",
    "\t\t\t\t\tnum_workers     = max(0, self.p.num_workers),\n",
    "\t\t\t\t\tcollate_fn      = dataset_class.collate_fn\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.data_iter = {\n",
    "\t\t\t'train':    \tget_data_loader(TrainDataset, 'train', \t    self.p.batch_size),\n",
    "\t\t\t'valid_head':   get_data_loader(TestDataset,  'valid_head', self.p.batch_size),\n",
    "\t\t\t'valid_tail':   get_data_loader(TestDataset,  'valid_tail', self.p.batch_size),\n",
    "\t\t\t'test_head':   \tget_data_loader(TestDataset,  'test_head',  self.p.batch_size),\n",
    "\t\t\t'test_tail':   \tget_data_loader(TestDataset,  'test_tail',  self.p.batch_size),\n",
    "\t\t}\n",
    "\n",
    "\t\tself.edge_index, self.edge_type = self.construct_adj(self.data['train'])\n",
    "\t\tself.rrg_edge_index, self.rrg_edge_type = self.construct_rrg(self.data['train'])\n",
    "\t\tself.features = self.concat_features(self.edge_index, self.edge_type)\n",
    "\t\tself.chequer_perm\t= self.get_chequer_perm()\n",
    "\n",
    "\tdef get_chequer_perm(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction to generate the chequer permutation required for InteractE model\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\t\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tent_perm  = np.int32([np.random.permutation(self.p.embed_dim) for _ in range(self.p.perm)])\n",
    "\t\trel_perm  = np.int32([np.random.permutation(self.p.embed_dim) for _ in range(self.p.perm)])\n",
    "\n",
    "\t\tcomb_idx = []\n",
    "\t\tfor k in range(self.p.perm):\n",
    "\t\t\ttemp = []\n",
    "\t\t\tent_idx, rel_idx = 0, 0\n",
    "\n",
    "\t\t\tfor i in range(self.p.k_h):\n",
    "\t\t\t\tfor j in range(self.p.k_w):\n",
    "\t\t\t\t\tif k % 2 == 0:\n",
    "\t\t\t\t\t\tif i % 2 == 0:\n",
    "\t\t\t\t\t\t\ttemp.append(ent_perm[k, ent_idx]); ent_idx += 1\n",
    "\t\t\t\t\t\t\ttemp.append(rel_perm[k, rel_idx]+self.p.embed_dim); rel_idx += 1\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\ttemp.append(rel_perm[k, rel_idx]+self.p.embed_dim); rel_idx += 1\n",
    "\t\t\t\t\t\t\ttemp.append(ent_perm[k, ent_idx]); ent_idx += 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif i % 2 == 0:\n",
    "\t\t\t\t\t\t\ttemp.append(rel_perm[k, rel_idx]+self.p.embed_dim); rel_idx += 1\n",
    "\t\t\t\t\t\t\ttemp.append(ent_perm[k, ent_idx]); ent_idx += 1\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\ttemp.append(ent_perm[k, ent_idx]); ent_idx += 1\n",
    "\t\t\t\t\t\t\ttemp.append(rel_perm[k, rel_idx]+self.p.embed_dim); rel_idx += 1\n",
    "\n",
    "\t\t\tcomb_idx.append(temp)\n",
    "\n",
    "\t\tchequer_perm = torch.LongTensor(np.int32(comb_idx)).to(self.device)\n",
    "\t\treturn chequer_perm\n",
    "\t\n",
    "\tdef concat_features(self, edge_index, edge_type):\n",
    "\t\t\tfeatures = self.compute_node_features(edge_index, edge_type, self.p.num_ent).to(self.device)\n",
    "\t\t\tprint(\"features:\", features)\n",
    "\t\t\tprint(\"features:\", features.size())\n",
    "\t\t\treturn features\n",
    "\n",
    "\tdef construct_adj(self, data):\n",
    "\t\t\tedge_index, edge_type = [], []\n",
    "\t\t\tedge_type_count = {} \n",
    "\n",
    "\t\t\tsub_unique = {}\n",
    "\t\t\tobj_unique = {}\n",
    "\n",
    "\t\t\tfor sub, rel, obj in data:\n",
    "\t\t\t\tedge_index.append((sub, obj))\n",
    "\t\t\t\tedge_type.append(rel)\n",
    "\t\t\t\tedge_type_count[rel] = edge_type_count.get(rel, 0) + 1\n",
    "\n",
    "\t\t\t\tif rel not in sub_unique:\n",
    "\t\t\t\t\tsub_unique[rel] = set()\n",
    "\t\t\t\tif rel not in obj_unique:\n",
    "\t\t\t\t\tobj_unique[rel] = set()\n",
    "\t\t\t\tsub_unique[rel].add(sub)\n",
    "\t\t\t\tobj_unique[rel].add(obj)\n",
    "\n",
    "\n",
    "\t\t\tfor sub, rel, obj in data:\n",
    "\t\t\t\tinverse_rel = rel + self.p.num_rel\n",
    "\t\t\t\tedge_index.append((obj, sub))\n",
    "\t\t\t\tedge_type.append(inverse_rel)\n",
    "\t\t\t\tedge_type_count[inverse_rel] = edge_type_count.get(inverse_rel, 0) + 1\n",
    "\n",
    "\t\t\t\tif inverse_rel not in sub_unique:\n",
    "\t\t\t\t\tsub_unique[inverse_rel] = set()\n",
    "\t\t\t\tif inverse_rel not in obj_unique:\n",
    "\t\t\t\t\tobj_unique[inverse_rel] = set()\n",
    "\t\t\t\tsub_unique[inverse_rel].add(obj)\n",
    "\t\t\t\tobj_unique[inverse_rel].add(sub)\n",
    "\n",
    "\t\t\tedge_index = torch.LongTensor(edge_index).to(self.device).t()\n",
    "\t\t\tedge_type = torch.LongTensor(edge_type).to(self.device)\n",
    "\t\t\treturn edge_index, edge_type\n",
    "\t\n",
    "\tdef construct_rrg(self, data):\n",
    "\t\tnum_relations = self.p.num_rel\n",
    "\n",
    "\t\thead_rel_dict = ddict(set)  # {ent: {rel of triples having ent as head}}\n",
    "\t\ttail_rel_dict = ddict(set)  # {ent: {rel of triples having ent as tail}}\n",
    "\t\tent_set = set()\n",
    "\n",
    "\t\tnew_triple = []\n",
    "\t\tnew_triple_text = set()\n",
    "\n",
    "\t\tfor triple in data:\n",
    "\t\t\thead, rel, tail = triple\n",
    "\t\t\tent_set.update([head, tail])\n",
    "\t\t\thead_rel_dict[head].add(rel)\n",
    "\t\t\ttail_rel_dict[tail].add(rel)\n",
    "\t\t\t\n",
    "\t\tfor ent in ent_set:\n",
    "\t\t\tas_tail_set = head_rel_dict[ent] \n",
    "\t\t\tas_head_set = tail_rel_dict[ent] \n",
    "\n",
    "\t\t\tfor ele in as_tail_set:\n",
    "\t\t\t\tfor ele2 in as_head_set:\n",
    "\t\t\t\t\tstr_key = \"{}_{}_{}\".format(ele, ent, ele2)\n",
    "\t\t\t\t\tif (ele % num_relations) == (ele2 % num_relations):\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\tif str_key not in new_triple_text:\n",
    "\t\t\t\t\t\tnew_triple_text.add(str_key)\n",
    "\t\t\t\t\t\tnew_triple.append([ele, ent, ele2])\n",
    "\t\tedge_count = {}\n",
    "\t\t\n",
    "\t\tfor sub, rel, obj in new_triple:\n",
    "\t\t\tedge = (sub, obj)\n",
    "\t\t\tedge_count[edge] = edge_count.get(edge, 0) + 1\n",
    "\t\tfor sub, rel, obj in new_triple:\n",
    "\t\t\tedge = (sub + num_relations, obj + num_relations)\n",
    "\t\t\tedge_count[edge] = edge_count.get(edge, 0) + 1\n",
    "\n",
    "\t\tedge_index = []\n",
    "\t\tedge_weight = []\n",
    "\t\tfor edge, count in edge_count.items():\n",
    "\t\t\tedge_index.append(edge)\n",
    "\t\t\tedge_weight.append(count)  \n",
    "\n",
    "\t\tedge_index = torch.LongTensor(edge_index).to(self.device).t()\n",
    "\t\tedge_weight = torch.LongTensor(edge_weight).to(self.device)\n",
    "\n",
    "\t\tmin_weight = torch.min(edge_weight[edge_weight > 0])\n",
    "\t\tlog_weights = torch.log(edge_weight + min_weight/10)\n",
    "\t\tedge_weight = (log_weights - log_weights.min()) / (log_weights.max() - log_weights.min())\n",
    "\n",
    "\t\treturn edge_index, edge_weight\n",
    "\n",
    "\n",
    "\tdef compute_node_features(self, edge_index, edge_type, num_nodes):\n",
    "\t\tG = nx.Graph()\n",
    "\t\tG.add_nodes_from(range(num_nodes))\n",
    "\t\tedges = edge_index.t().tolist()\n",
    "\t\tfor i, (u, v) in enumerate(edges):\n",
    "\t\t\trel = edge_type[i].item() \n",
    "\t\t\tG.add_edge(u, v, relation=rel)\n",
    "\t\tG.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "\t\tsecond_neighbors_dict = {}\n",
    "\t\tpath_relations = ddict(lambda: ddict(list))\n",
    "\n",
    "\t\tfor n in G.nodes():\n",
    "\t\t\tneighbors = set(G.neighbors(n))\n",
    "\t\t\tsecond_neighbors = set()\n",
    "\t\t\tfor neighbor in neighbors:\n",
    "\t\t\t\tfor second_neighbor in G.neighbors(neighbor):\n",
    "\t\t\t\t\tif second_neighbor != n and second_neighbor not in neighbors:\n",
    "\t\t\t\t\t\tsecond_neighbors.add(second_neighbor)\n",
    "\t\t\t\t\t\trel1 = G.edges[n, neighbor]['relation']\n",
    "\t\t\t\t\t\trel2 = G.edges[neighbor, second_neighbor]['relation']\n",
    "\t\t\t\t\t\tpath_relations[n][second_neighbor].append((rel1, rel2))\n",
    "\t\t\tsecond_neighbors_dict[n] = second_neighbors\n",
    "\t\tnbr_deg_stats = {}\n",
    "\t\tfor n in G.nodes():\n",
    "\t\t\tneighbors = list(G.neighbors(n))\n",
    "\t\t\tif not neighbors:\n",
    "\t\t\t\tmin_deg = 0.0\n",
    "\t\t\t\tavg_deg = 0.0\n",
    "\t\t\t\tmax_deg = 0.0\n",
    "\t\t\telse:\n",
    "\t\t\t\tdegrees = [G.degree[neighbor] for neighbor in neighbors]\n",
    "\t\t\t\tmin_deg = min(degrees)\n",
    "\t\t\t\tavg_deg = sum(degrees) / len(degrees)\n",
    "\t\t\t\tmax_deg = max(degrees)\n",
    "\t\t\tnbr_deg_stats[n] = (min_deg, avg_deg, max_deg)\n",
    "\n",
    "\t\tsecond_hop_deg_stats = {}\n",
    "\t\tfor n in G.nodes():\n",
    "\t\t\tsecond_neighbors = second_neighbors_dict.get(n, set())\n",
    "\t\t\tif not second_neighbors:\n",
    "\t\t\t\tmin_deg = 0.0\n",
    "\t\t\t\tavg_deg = 0.0\n",
    "\t\t\t\tmax_deg = 0.0\n",
    "\t\t\telse:\n",
    "\t\t\t\tdegrees = [G.degree[sn] for sn in second_neighbors]\n",
    "\t\t\t\tmin_deg = min(degrees)\n",
    "\t\t\t\tavg_deg = sum(degrees) / len(degrees)\n",
    "\t\t\t\tmax_deg = max(degrees)\n",
    "\t\t\tsecond_hop_deg_stats[n] = (min_deg, avg_deg, max_deg)\n",
    "\n",
    "\t\tfeatures = [\n",
    "\t\t\t[G.degree(n),\n",
    "\t\t\tnbr_deg_stats[n][0], nbr_deg_stats[n][1], nbr_deg_stats[n][2],\n",
    "\t\t\tsecond_hop_deg_stats[n][0], second_hop_deg_stats[n][1], second_hop_deg_stats[n][2]]\n",
    "\t\t\tfor n in range(num_nodes)\n",
    "\t\t]\n",
    "\n",
    "\n",
    "\t\tdegrees = dict(G.degree())\n",
    "\t\tgravity_scores = []\n",
    "\t\tfor u in G.nodes():\n",
    "\t\t\tscore = 0.0\n",
    "\t\t\tpaths = nx.single_source_shortest_path_length(G, u, cutoff=5)\n",
    "\t\t\tfor v, d in paths.items():\n",
    "\t\t\t\tif u == v or d == 0: continue\n",
    "\t\t\t\tscore += (degrees[u] * degrees[v]) / (d ** 2)\n",
    "\t\t\tgravity_scores.append(score)\n",
    "\t\tgravity_scores = torch.tensor(gravity_scores, dtype=torch.float)\n",
    "\n",
    "\n",
    "\t\tdef array_norm(array):\n",
    "\t\t\tmax_val = torch.max(array).item()\n",
    "\t\t\tmin_val = torch.min(array).item()\n",
    "\t\t\tif min_val < 0: min_val = 0\n",
    "\t\t\treturn (array - min_val)/(max_val - min_val) if max_val != min_val else torch.zeros_like(array)\n",
    "\n",
    "\t\tfeatures = torch.tensor(features, dtype=torch.float)\n",
    "\t\tprocessed = [array_norm(features[:, col]) for col in range(features.size(1))]\n",
    "\t\tprocessed.append(array_norm(gravity_scores))\n",
    "\n",
    "\n",
    "\t\trel_counts = ddict(int)\n",
    "\t\trel_degree_sums = ddict(float)\n",
    "\t\tfor u, v, data in G.edges(data=True):\n",
    "\t\t\trel = data['relation']\n",
    "\t\t\trel_counts[rel] += 1\n",
    "\t\t\trel_degree_sums[rel] += degrees[u] + degrees[v]\n",
    "\n",
    "\t\tavg_degree_per_rel = {}\n",
    "\t\tfor rel in rel_counts:\n",
    "\t\t\tavg_degree_per_rel[rel] = rel_degree_sums[rel] / (2 * rel_counts[rel])\n",
    "\n",
    "\n",
    "\t\tmax_count = max(rel_counts.values()) if rel_counts else 1\n",
    "\t\tmin_count = min(rel_counts.values()) if rel_counts else 0\n",
    "\t\tmax_avg_deg = max(avg_degree_per_rel.values()) if avg_degree_per_rel else 1\n",
    "\t\tmin_avg_deg = min(avg_degree_per_rel.values()) if avg_degree_per_rel else 0\n",
    "\n",
    "\t\trel_embeddings = {}\n",
    "\t\tfor rel in rel_counts:\n",
    "\t\t\tnorm_count = (rel_counts[rel] - min_count) / (max_count - min_count) if max_count != min_count else 0.0\n",
    "\t\t\tnorm_avg_deg = (avg_degree_per_rel[rel] - min_avg_deg) / (max_avg_deg - min_avg_deg) if max_avg_deg != min_avg_deg else 0.0\n",
    "\t\t\trel_embeddings[rel] = (norm_count, norm_avg_deg)\n",
    "\n",
    "\n",
    "\t\trelation_weighted_features = []\n",
    "\t\tfor n in G.nodes():\n",
    "\t\t\tsum_embed = [0.0, 0.0]\n",
    "\t\t\tcount = 0\n",
    "\t\t\tfor neighbor in G.neighbors(n):\n",
    "\t\t\t\tedge_data = G.get_edge_data(n, neighbor)\n",
    "\t\t\t\trel = edge_data['relation']\n",
    "\t\t\t\tembed = rel_embeddings.get(rel, (0.0, 0.0))\n",
    "\t\t\t\tdeg = degrees[neighbor]\n",
    "\t\t\t\tsum_embed[0] += deg * embed[0]\n",
    "\t\t\t\tsum_embed[1] += deg * embed[1]\n",
    "\t\t\t\tcount += 1\n",
    "\t\t\tavg_embed = [s/count if count else 0.0 for s in sum_embed]\n",
    "\t\t\trelation_weighted_features.append(avg_embed)\n",
    "\n",
    "\t\trelation_weighted_features = torch.tensor(relation_weighted_features, dtype=torch.float)\n",
    "\t\tprocessed_rel_feat1 = array_norm(relation_weighted_features[:, 0])\n",
    "\t\tprocessed_rel_feat2 = array_norm(relation_weighted_features[:, 1])\n",
    "\t\tprocessed.extend([processed_rel_feat1, processed_rel_feat2])\n",
    "\n",
    "\n",
    "\t\tneighbor_deg_features = []\n",
    "\t\tfor u in G.nodes():\n",
    "\t\t\tdirect_neighbors = set(G.neighbors(u))\n",
    "\t\t\tsecond_neighbors = second_neighbors_dict.get(u, set())\n",
    "\t\t\tall_neighbors = direct_neighbors.union(second_neighbors)\n",
    "\t\t\tneighbors = list(all_neighbors)\n",
    "\t\t\t\n",
    "\t\t\tneighbor_scores = []\n",
    "\t\t\tfor v in neighbors:\n",
    "\t\t\t\tif v in direct_neighbors:\n",
    "\t\t\t\t\tedge_data = G.get_edge_data(u, v)\n",
    "\t\t\t\t\trel = edge_data['relation']\n",
    "\t\t\t\t\tembed = rel_embeddings.get(rel, (0.0, 0.0))\n",
    "\t\t\t\t\tavg_weight = (embed[0] + embed[1]) / 2\n",
    "\t\t\t\telse:  \n",
    "\t\t\t\t\trelations_list = path_relations[u].get(v, [])\n",
    "\t\t\t\t\tif not relations_list:\n",
    "\t\t\t\t\t\tavg_weight = 0.0\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttotal_avg = 0.0\n",
    "\t\t\t\t\t\tfor rel1, rel2 in relations_list:\n",
    "\t\t\t\t\t\t\tembed1 = rel_embeddings.get(rel1, (0.0, 0.0))\n",
    "\t\t\t\t\t\t\tembed2 = rel_embeddings.get(rel2, (0.0, 0.0))\n",
    "\t\t\t\t\t\t\tavg_embed0 = (embed1[0] + embed2[0]) / 2\n",
    "\t\t\t\t\t\t\tavg_embed1 = (embed1[1] + embed2[1]) / 2\n",
    "\t\t\t\t\t\t\tcombined_avg = (avg_embed0 + avg_embed1)/2 \n",
    "\t\t\t\t\t\t\ttotal_avg += combined_avg\n",
    "\t\t\t\t\t\tavg_weight = total_avg / len(relations_list)\n",
    "\t\t\t\tscore = gravity_scores[v].item() * avg_weight\n",
    "\t\t\t\tneighbor_scores.append((v, score))\n",
    "\t\t\t\n",
    "\t\t\tsorted_neighbors = sorted(neighbor_scores, key=lambda x: -x[1])\n",
    "\t\t\ttop_10 = [v for v, _ in sorted_neighbors[:10]]\n",
    "\t\t\t\n",
    "\t\t\tdegs = [degrees[v] for v in top_10]\n",
    "\t\t\tdegs += [0.0] * (10 - len(degs))\n",
    "\t\t\tneighbor_deg_features.append(degs)\n",
    "\n",
    "\t\tneighbor_deg_features = torch.tensor(neighbor_deg_features, dtype=torch.float)\n",
    "\t\tprocessed_neighbor_degs = torch.stack([array_norm(neighbor_deg_features[:, i]) for i in range(10)], dim=1)\n",
    "\t\t\n",
    "\t\tfinal_features = torch.cat([torch.stack(processed, dim=1), processed_neighbor_degs], dim=1)\n",
    "\t\treturn final_features\n",
    "\t\n",
    "\n",
    "\tdef add_model(self, model, score_func):\n",
    "\n",
    "\t\tmodel_name = '{}_{}'.format(model, score_func)\n",
    "\n",
    "\t\tif model_name.lower()\t== 'compgcn_conve': \t\t\tmodel = CompGCN_ConvE(self.edge_index, self.edge_type, self.rrg_edge_index, self.rrg_edge_type, self.features,self.chequer_perm, params=self.p)\n",
    "\t\telif model_name.lower()\t== 'compgcn_interacte': \t\tmodel = CompGCN_InteractE(self.edge_index, self.edge_type, self.rrg_edge_index, self.rrg_edge_type, self.features,self.chequer_perm, params=self.p)\n",
    "\t\telse: raise NotImplementedError\n",
    "\n",
    "\t\tmodel.to(self.device)\n",
    "\t\treturn model\n",
    "\n",
    "\tdef add_optimizer(self, parameters):\n",
    "\n",
    "\n",
    "\t\treturn torch.optim.Adam(parameters, lr=self.p.lr, weight_decay=self.p.l2)\n",
    "\t\n",
    "\tdef read_batch(self, batch, split):\n",
    "\n",
    "\t\tif split == 'train':\n",
    "\t\t\ttriple, label = [ _.to(self.device) for _ in batch]\n",
    "\t\t\treturn triple[:, 0], triple[:, 1], triple[:, 2], label\n",
    "\t\telse:\n",
    "\t\t\ttriple, label = [ _.to(self.device) for _ in batch]\n",
    "\t\t\treturn triple[:, 0], triple[:, 1], triple[:, 2], label\n",
    "\t\t\n",
    "\tdef save_model(self, save_path):\n",
    "\n",
    "\t\tdir_path = os.path.dirname(save_path)\n",
    "\t\tif not os.path.exists(dir_path):\n",
    "\t\t\tos.makedirs(dir_path)\n",
    "\n",
    "\t\tstate = {\n",
    "\t\t\t'state_dict'\t: self.model.state_dict(),\n",
    "\t\t\t'best_val'\t: self.best_val,\n",
    "\t\t\t'best_epoch'\t: self.best_epoch,\n",
    "\t\t\t'optimizer'\t: self.optimizer.state_dict(),\n",
    "\t\t\t'args'\t\t: vars(self.p)\n",
    "\t\t}\n",
    "\t\ttorch.save(state, save_path)\n",
    "\n",
    "\tdef load_model(self, load_path):\n",
    "\n",
    "\t\tstate\t\t\t= torch.load(load_path)\n",
    "\t\tstate_dict\t\t= state['state_dict']\n",
    "\t\tself.best_val\t\t= state['best_val']\n",
    "\t\tself.best_val_mrr\t= self.best_val['mrr'] \n",
    "\n",
    "\t\tself.model.load_state_dict(state_dict)\n",
    "\t\tself.optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "\tdef evaluate(self, split, epoch):\n",
    "\n",
    "\t\tleft_results  = self.predict(split=split, mode='tail_batch')\n",
    "\t\tright_results = self.predict(split=split, mode='head_batch')\n",
    "\t\tresults       = get_combined_results(left_results, right_results)\n",
    "\t\tself.logger.info('[Epoch {} {}]: MRR: Tail : {:.5}, Head : {:.5}, Avg : {:.5}'.format(epoch, split, results['left_mrr'], results['right_mrr'], results['mrr']))\n",
    "\t\treturn results\n",
    "\n",
    "\tdef predict(self, split='valid', mode='tail_batch'):\n",
    "\n",
    "\t\tself.model.eval()\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tresults = {}\n",
    "\t\t\ttrain_iter = iter(self.data_iter['{}_{}'.format(split, mode.split('_')[0])])\n",
    "\n",
    "\t\t\tfor step, batch in enumerate(train_iter):\n",
    "\t\t\t\tsub, rel, obj, label\t= self.read_batch(batch, split)\n",
    "\t\t\t\tpred\t\t\t= self.model.forward(sub, rel)\n",
    "\t\t\t\tb_range\t\t\t= torch.arange(pred.size()[0], device=self.device)\n",
    "\t\t\t\ttarget_pred\t\t= pred[b_range, obj]\n",
    "\t\t\t\tpred \t\t\t= torch.where(label.bool(), -torch.ones_like(pred) * 10000000, pred)\n",
    "\t\t\t\tpred[b_range, obj] \t= target_pred\n",
    "\t\t\t\tranks\t\t\t= 1 + torch.argsort(torch.argsort(pred, dim=1, descending=True), dim=1, descending=False)[b_range, obj]\n",
    "\n",
    "\t\t\t\tranks \t\t\t= ranks.float()\n",
    "\t\t\t\tresults['count']\t= torch.numel(ranks) \t\t+ results.get('count', 0.0)\n",
    "\t\t\t\tresults['mr']\t\t= torch.sum(ranks).item() \t+ results.get('mr',    0.0)\n",
    "\t\t\t\tresults['mrr']\t\t= torch.sum(1.0/ranks).item()   + results.get('mrr',   0.0)\n",
    "\t\t\t\tfor k in range(10):\n",
    "\t\t\t\t\tresults['hits@{}'.format(k+1)] = torch.numel(ranks[ranks <= (k+1)]) + results.get('hits@{}'.format(k+1), 0.0)\n",
    "\n",
    "\t\t\t\tif step % 100 == 0:\n",
    "\t\t\t\t\tself.logger.info('[{}, {} Step {}]\\t{}'.format(split.title(), mode.title(), step, self.p.name))\n",
    "\n",
    "\t\treturn results\n",
    "\n",
    "\tdef run_epoch(self, epoch, val_mrr = 0):\n",
    "\n",
    "\t\tself.model.train()\n",
    "\t\tlosses = []\n",
    "\t\ttrain_iter = iter(self.data_iter['train'])\n",
    "\n",
    "\t\tfor step, batch in enumerate(train_iter):\n",
    "\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\tsub, rel, obj, label = self.read_batch(batch, 'train')\n",
    "\n",
    "\t\t\tpred\t= self.model.forward(sub, rel)\n",
    "\t\t\tloss\t= self.model.loss(pred, label)\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\t\t\ttorch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # 裁剪梯度\n",
    "\t\t\tself.optimizer.step()\n",
    "\t\t\tlosses.append(loss.item())\n",
    "\n",
    "\t\t\tif step % 100 == 0:\n",
    "\t\t\t\tself.logger.info('[E:{}| {}]: Train Loss:{:.5},  Val MRR:{:.5}\\t{}'.format(epoch, step, np.mean(losses), self.best_val_mrr, self.p.name))\n",
    "\n",
    "\t\tloss = np.mean(losses)\n",
    "\t\tself.logger.info('[Epoch:{}]:  Training Loss:{:.4}\\n'.format(epoch, loss))\n",
    "\t\treturn loss\n",
    "\t\n",
    "\tdef fit(self):\n",
    "\t\t\n",
    "\t\tself.best_val_mrr, self.best_val, self.best_epoch, val_mrr = 0., {}, 0, 0.\n",
    "\t\tsave_path = os.path.join('./checkpoints', self.p.name)\n",
    "\n",
    "\t\tos.makedirs(self.p.log_dir, exist_ok=True) \n",
    "\t\tmetrics_file = os.path.join(self.p.log_dir, '8_2_training_metrics.csv')\n",
    "\t\t\n",
    "\t\tif not os.path.exists(metrics_file):\n",
    "\t\t\twith open(metrics_file, 'a') as f:\n",
    "\t\t\t\tf.write('epoch,train_loss,val_mr,val_mrr,val_hits@1,val_hits@3,val_hits@10\\n')\n",
    "\n",
    "\t\tif self.p.restore:\n",
    "\t\t\tself.load_model(save_path)\n",
    "\t\t\tself.logger.info('Successfully Loaded previous model')\n",
    "\n",
    "\t\tkill_cnt = 0\n",
    "\t\tfor epoch in range(self.p.max_epochs):\n",
    "\t\t\ttrain_loss  = self.run_epoch(epoch, val_mrr)\n",
    "\t\t\tval_results = self.evaluate('valid', epoch)\n",
    "\n",
    "\t\t\twith open(metrics_file, 'a') as f:\n",
    "\t\t\t\tline = f\"{epoch},{train_loss:.5f},{val_results['mr']:.5f},\" \\\n",
    "\t\t\t\t\tf\"{val_results['mrr']:.5f},{val_results['hits@1']:.5f},\" \\\n",
    "\t\t\t\t\tf\"{val_results['hits@3']:.5f},{val_results['hits@10']:.5f}\\n\"\n",
    "\t\t\t\tf.write(line)\n",
    "\n",
    "\t\t\tif val_results['mrr'] > self.best_val_mrr:\n",
    "\t\t\t\tself.best_val     = val_results\n",
    "\t\t\t\tself.best_val_mrr = val_results['mrr']\n",
    "\t\t\t\tself.best_epoch   = epoch\n",
    "\t\t\t\tself.save_model(save_path)\n",
    "\t\t\t\tkill_cnt = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tkill_cnt += 1\n",
    "\t\t\t\tif kill_cnt > 40: \n",
    "\t\t\t\t\tself.logger.info(\"Early Stopping!!\")\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\tself.logger.info('[Epoch {}]: Training Loss: {:.5}, Valid MRR: {:.5}\\n\\n'.format(epoch, train_loss, self.best_val_mrr))\n",
    "\n",
    "\t\tself.logger.info('Loading best model, Evaluating on Test data')\n",
    "\t\tself.load_model(save_path)\n",
    "\t\ttest_results = self.evaluate('test', epoch)\n",
    "\t\treturn test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-16 00:46:25,867 - [INFO] - {'dataset': 'UMLS', 'k_w': 10, 'k_h': 20, 'embed_dim': 200, 'batch_size': 256, 'num_workers': 0, 'name': 'testrun', 'log_dir': './', 'config_dir': './config/', 'gpu': '0', 'model': 'compgcn', 'score_func': 'interacte', 'gcn_layer': 1, 'init_dim': 200, 'dropout': 0.1, 'bias': False, 'num_filt': 128, 'hid_drop': 0.1, 'hid_drop2': 0.3, 'feat_drop': 0.3, 'ker_sz': 7, 'lr': 0.001, 'l2': 0.0, 'restore': False, 'max_epochs': 500, 'lbl_smooth': 0.1, 'gcn_dim': 200, 'gamma': 9.0, 'perm': 2}\n",
      "{'batch_size': 256,\n",
      " 'bias': False,\n",
      " 'config_dir': './config/',\n",
      " 'dataset': 'UMLS',\n",
      " 'dropout': 0.1,\n",
      " 'embed_dim': 200,\n",
      " 'feat_drop': 0.3,\n",
      " 'gamma': 9.0,\n",
      " 'gcn_dim': 200,\n",
      " 'gcn_layer': 1,\n",
      " 'gpu': '0',\n",
      " 'hid_drop': 0.1,\n",
      " 'hid_drop2': 0.3,\n",
      " 'init_dim': 200,\n",
      " 'k_h': 20,\n",
      " 'k_w': 10,\n",
      " 'ker_sz': 7,\n",
      " 'l2': 0.0,\n",
      " 'lbl_smooth': 0.1,\n",
      " 'log_dir': './',\n",
      " 'lr': 0.001,\n",
      " 'max_epochs': 500,\n",
      " 'model': 'compgcn',\n",
      " 'name': 'testrun',\n",
      " 'num_filt': 128,\n",
      " 'num_workers': 0,\n",
      " 'perm': 2,\n",
      " 'restore': False,\n",
      " 'score_func': 'interacte'}\n",
      "num entities:  135\n",
      "num relations:  46\n",
      "features: tensor([[0.8435, 0.3333, 0.4296,  ..., 0.6250, 0.4375, 0.4643],\n",
      "        [0.8174, 0.3333, 0.4342,  ..., 0.4750, 0.5938, 0.6429],\n",
      "        [0.8174, 0.2821, 0.4217,  ..., 0.5000, 0.4375, 0.4643],\n",
      "        ...,\n",
      "        [0.0348, 0.1026, 0.3634,  ..., 0.6250, 0.7188, 0.6429],\n",
      "        [0.0000, 0.6923, 0.8140,  ..., 0.4750, 0.5938, 0.5357],\n",
      "        [0.0261, 0.2308, 0.5857,  ..., 0.5750, 0.5938, 0.6429]],\n",
      "       device='cuda:0')\n",
      "features: torch.Size([135, 20])\n",
      "2025-06-16 00:46:37,823 - [INFO] - [E:0| 0]: Train Loss:2.5558,  Val MRR:0.0\ttestrun\n",
      "2025-06-16 00:46:39,544 - [INFO] - [Epoch:0]:  Training Loss:1.092\n",
      "\n",
      "2025-06-16 00:46:39,649 - [INFO] - [Valid, Tail_Batch Step 0]\ttestrun\n",
      "2025-06-16 00:46:39,871 - [INFO] - [Valid, Head_Batch Step 0]\ttestrun\n",
      "2025-06-16 00:46:40,010 - [INFO] - [Epoch 0 valid]: MRR: Tail : 0.18753, Head : 0.16885, Avg : 0.17819\n",
      "2025-06-16 00:46:40,894 - [INFO] - [Epoch 0]: Training Loss: 1.0919, Valid MRR: 0.17819\n",
      "\n",
      "\n",
      "2025-06-16 00:46:41,196 - [INFO] - [E:1| 0]: Train Loss:0.45602,  Val MRR:0.17819\ttestrun\n",
      "2025-06-16 00:46:42,770 - [INFO] - [Epoch:1]:  Training Loss:0.47\n",
      "\n",
      "2025-06-16 00:46:42,845 - [INFO] - [Valid, Tail_Batch Step 0]\ttestrun\n",
      "2025-06-16 00:46:43,098 - [INFO] - [Valid, Head_Batch Step 0]\ttestrun\n",
      "2025-06-16 00:46:43,290 - [INFO] - [Epoch 1 valid]: MRR: Tail : 0.20413, Head : 0.18423, Avg : 0.19418\n",
      "2025-06-16 00:46:44,217 - [INFO] - [Epoch 1]: Training Loss: 0.47005, Valid MRR: 0.19418\n",
      "\n",
      "\n",
      "2025-06-16 00:46:44,566 - [INFO] - [E:2| 0]: Train Loss:0.45973,  Val MRR:0.19418\ttestrun\n",
      "2025-06-16 00:46:46,465 - [INFO] - [Epoch:2]:  Training Loss:0.4114\n",
      "\n",
      "2025-06-16 00:46:46,572 - [INFO] - [Valid, Tail_Batch Step 0]\ttestrun\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_43672\\1230976240.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_43672\\3313181433.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    561\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                         \u001b[0mtrain_loss\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_mrr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m                         \u001b[0mval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m                         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_43672\\3313181433.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, split, epoch)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m                 \u001b[0mleft_results\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tail_batch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m                 \u001b[0mright_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'head_batch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0mresults\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[0mget_combined_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_43672\\3313181433.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, split, mode)\u001b[0m\n\u001b[0;32m    507\u001b[0m                                 \u001b[0mranks\u001b[0m                   \u001b[1;33m=\u001b[0m \u001b[0mranks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m                                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mranks\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;33m+\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m                                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mr'\u001b[0m\u001b[1;33m]\u001b[0m           \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mranks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;33m+\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mr'\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m                                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mrr'\u001b[0m\u001b[1;33m]\u001b[0m          \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mranks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;33m+\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mrr'\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m                                 \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-data',\t\tdest='dataset',         default='UMLS',                                                         help='Dataset to use, default: FB15k-237')\n",
    "parser.add_argument('-k_w',\t  \t    dest='k_w', \t\t    default=10,   \t            type=int, \t                                help='Decoder: k_w')\n",
    "parser.add_argument('-k_h',\t  \t    dest='k_h', \t\t    default=20,   \t            type=int, \t                                help='Decoder: k_h')\n",
    "parser.add_argument('-embed_dim',\tdest='embed_dim', \t    default=200,                type=int, \t                                help='Embedding dimension to give as input to score function')\n",
    "parser.add_argument('-batch',       dest='batch_size',      default=256,                type=int,                                   help='Batch size')\n",
    "parser.add_argument('-num_workers',\ttype=int,               default=0,                                                              help='Number of processes to construct batches')\n",
    "parser.add_argument('-name',        default='testrun',                                                                              help='Set run name for saving/restoring models')\n",
    "parser.add_argument('-logdir',      dest='log_dir',         default='./',                                                           help='Log directory')\n",
    "parser.add_argument('-config',      dest='config_dir',      default='./config/',                                                    help='Config directory')\n",
    "parser.add_argument('-gpu',         type=str,               default='0',                                                            help='Set GPU Ids : Eg: For CPU = -1, For Single GPU = 0')\n",
    "parser.add_argument('-model',       dest='model',           default='compgcn',                                                      help='Model Name')\n",
    "parser.add_argument('-score_func',  dest='score_func',      default='interacte',                                                    help='Score Function for Link prediction')\n",
    "parser.add_argument('-gcn_layer',   dest='gcn_layer',       default=1,                  type=int,                                   help='Number of GCN Layers to use')\n",
    "parser.add_argument('-init_dim',    dest='init_dim',        default=200,                type=int,                                   help='Initial dimension size for entities and relations')\n",
    "parser.add_argument('-gcn_drop',    dest='dropout',         default=0.1,                type=float,                                 help='Dropout to use in GCN Layer')\n",
    "parser.add_argument('-bias',        dest='bias',            action='store_true',                                                    help='Whether to use bias in the model')\n",
    "parser.add_argument('-num_filt',    dest='num_filt',        default=128,                type=int,                                   help='Decoder: Number of filters in convolution')\n",
    "parser.add_argument('-hid_drop',    dest='hid_drop',        default=0.1,                type=float,                                 help='Dropout after GCN')\n",
    "parser.add_argument('-hid_drop2',   dest='hid_drop2',       default=0.3,                type=float,                                 help='Decoder: Hidden dropout')\n",
    "parser.add_argument('-feat_drop',   dest='feat_drop',       default=0.3,                type=float,                                 help='Decoder: Feature Dropout')\n",
    "parser.add_argument('-ker_sz',      dest='ker_sz',          default=7,                  type=int,                                   help='Decoder: Kernel size to use')\n",
    "parser.add_argument('-lr',          type=float,             default=0.001,                                                          help='Starting Learning Rate')\n",
    "parser.add_argument('-l2',          type=float,             default=0.0,                                                            help='L2 Regularization for Optimizer')\n",
    "parser.add_argument('-restore',     dest='restore',         action='store_true',                                                    help='Restore from the previously saved model')\n",
    "parser.add_argument('-epoch',\t\tdest='max_epochs', \t    type=int,                   default=500,  \t                            help='Number of epochs')\n",
    "parser.add_argument('-lbl_smooth',  dest='lbl_smooth',\t    type=float,                 default=0.1,\t                            help='Label Smoothing')\n",
    "parser.add_argument('-gcn_dim',\t  \tdest='gcn_dim', \t    default=200,   \t            type=int, \t                                help='Number of hidden units in GCN')\n",
    "\n",
    "parser.add_argument('-gamma',\t\ttype=float,             default=9.0,\t\t\thelp='Margin')\n",
    "parser.add_argument('-perm',\t  \t    dest='perm', \t\t    default=2,   \t            type=int, \t                                help='num_perm')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "model = Runner(args)\n",
    "\n",
    "model.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
